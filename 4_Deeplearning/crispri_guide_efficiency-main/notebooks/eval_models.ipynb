{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## packages\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from src.utils import get_data, get_colnames\n",
    "from src.eval import eval_top_n_guides_genewise, eval_top_n_guides_modelwise, calculate_ranking, plot_ranking, create_challengeR_dataset\n",
    "\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set seeds\n",
    "seed = 5555\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "## get time stamp\n",
    "date = datetime.now()\n",
    "date = \"{}-{}-{}\".format(date.year, date.month, date.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## files and directories\n",
    "date_model_trained = \"2021-4-13\"\n",
    "\n",
    "input_predictions = output_models = \"/storage/groups/haicu/workspace/crispri/models/\" + date_model_trained + \"/\"\n",
    "output_performance = \"../reports/performance_eval/\" + date + \"/\"\n",
    "output_plots = \"../reports/plots_eval/\" + date + \"/\"\n",
    "\n",
    "os.makedirs(os.path.dirname(output_performance), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(output_plots), exist_ok=True)\n",
    "\n",
    "file_data_wang = '../datasets/data_wang.pickle'\n",
    "file_data_rousset_E18 = '../datasets/data_rousset_E18.pickle'\n",
    "file_data_rousset_E75 = '../datasets/data_rousset_E75.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup parameters\n",
    "top_n_guides_list = [1,3] \n",
    "p_value_thr = 0.05\n",
    "\n",
    "#datasets_models_trained = [\"wang_orig_guide\", \"rousset_E18_orig_guide\", \"rousset_E75_orig_guide\", \"wang_rousset_E18_orig_guide\", \"wang_rousset_E75_orig_guide\", \"wang_rousset_E18_rousset_E75_orig_guide\"]\n",
    "\n",
    "#datasets_models_trained = [\"wang_median-sub_guide\", \"rousset_E18_median-sub_guide\", \"rousset_E75_median-sub_guide\", \"wang_rousset_E18_median-sub_guide\", \"wang_rousset_E75_median-sub_guide\", \"wang_rousset_E18_rousset_E75_median-sub_guide\"]\n",
    "\n",
    "datasets_models_trained = [\"wang_rank_guide\", \"rousset_E18_rank_guide\", \"rousset_E75_rank_guide\", \"wang_rousset_E18_rank_guide\", \"wang_rousset_E75_rank_guide\", \"wang_rousset_E18_rousset_E75_rank_guide\"]\n",
    "\n",
    "models_trained = [\"elnet\", \"GBM\", \"1DCNN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of genes per dataset:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'wang': 244, 'rousset_E18': 147, 'rousset_E75': 137}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load data\n",
    "data_wang = get_data(file_data_wang)\n",
    "data_rousset_E18 = get_data(file_data_rousset_E18)\n",
    "data_rousset_E75 = get_data(file_data_rousset_E75)\n",
    "\n",
    "genes_of_datasets = {\"wang\": sorted(data_wang[\"geneid\"].to_list()), \"rousset_E18\": sorted(data_rousset_E18[\"geneid\"].to_list()), \"rousset_E75\": sorted(data_rousset_E75[\"geneid\"].to_list())}\n",
    "print(\"number of genes per dataset:\")\n",
    "{k: len(set(v)) for k, v in genes_of_datasets.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: agg\n",
      "\n",
      "---\n",
      "calculate metrics and ranking for top 1 guides\n",
      "dataset: wang_rank_guide\n",
      "dataset: rousset_E18_rank_guide\n",
      "dataset: rousset_E75_rank_guide\n",
      "dataset: wang_rousset_E18_rank_guide\n",
      "dataset: wang_rousset_E75_rank_guide\n",
      "dataset: wang_rousset_E18_rousset_E75_rank_guide\n",
      "                                            elnet       GBM     1DCNN\n",
      "wang_rank_guide                          2.030702  2.008772  1.960526\n",
      "rousset_E18_rank_guide                   1.934783  2.184783  1.880435\n",
      "rousset_E75_rank_guide                   1.955556  2.155556  1.888889\n",
      "wang_rousset_E18_rank_guide              2.065789   1.97807   1.95614\n",
      "wang_rousset_E75_rank_guide              2.002193  2.046053  1.951754\n",
      "wang_rousset_E18_rousset_E75_rank_guide  1.991228  1.993421  2.015351\n",
      "\n",
      "---\n",
      "calculate metrics and ranking for top 3 guides\n",
      "dataset: wang_rank_guide\n",
      "dataset: rousset_E18_rank_guide\n",
      "dataset: rousset_E75_rank_guide\n",
      "dataset: wang_rousset_E18_rank_guide\n",
      "dataset: wang_rousset_E75_rank_guide\n",
      "dataset: wang_rousset_E18_rousset_E75_rank_guide\n",
      "                                            elnet       GBM     1DCNN\n",
      "wang_rank_guide                          1.975877  2.041667  1.982456\n",
      "rousset_E18_rank_guide                   1.891304  2.043478  2.065217\n",
      "rousset_E75_rank_guide                   2.044444  2.111111  1.844444\n",
      "wang_rousset_E18_rank_guide              1.949561  2.052632  1.997807\n",
      "wang_rousset_E75_rank_guide               1.97807  2.010965  2.010965\n",
      "wang_rousset_E18_rousset_E75_rank_guide  1.982456  2.078947  1.938596\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "\n",
    "colnames_predictions = [\"log2FC_target\",\"log2FC_predicted\",\"log2FC_original\"]\n",
    "colnames_metrics = [\"spearmanR\",\"performance_increase\",\"wilcoxon_p-value\"]\n",
    "\n",
    "# calculate metrics and ranking\n",
    "for n in range(len(top_n_guides_list)):\n",
    "    \n",
    "    top_n_guides = top_n_guides_list[n]\n",
    "    print(\"\\n---\")\n",
    "    print(\"calculate metrics and ranking for top \" + str(top_n_guides) + \" guides\")\n",
    "\n",
    "    number_sig_p_values_per_model_per_dataset = pd.DataFrame(columns = models_trained, index = datasets_models_trained)\n",
    "    ranking_per_model_per_dataset = pd.DataFrame(columns = models_trained, index = datasets_models_trained)\n",
    "    ranking_per_gene_per_model_mean = pd.DataFrame(columns = models_trained, index = datasets_models_trained)\n",
    "    mean_FC_top_n_guides_per_gene_per_model_per_dataset = []\n",
    "    mean_mse_per_gene_per_model_per_dataset = []\n",
    "    spearmanR_per_gene_per_model_per_dataset = []\n",
    "    \n",
    "    for dataset in datasets_models_trained:\n",
    "        print(\"dataset: \" + dataset)\n",
    "           \n",
    "        if \"wang\" in dataset:\n",
    "            genes = genes_of_datasets[\"wang\"]\n",
    "            \n",
    "        elif \"rousset_E18\" in dataset:\n",
    "            genes = genes_of_datasets[\"rousset_E18\"]\n",
    "                \n",
    "        elif \"rousset_E75\" in dataset:\n",
    "            genes = genes_of_datasets[\"rousset_E75\"] \n",
    "        genes_unique = sorted(list(set(genes)))\n",
    "        \n",
    "        #assemble all predictions of all models and plot summary plots\n",
    "        filename_plots = output_plots + dataset + \"_summary.pdf\"\n",
    "        pp = PdfPages(filename_plots)\n",
    "    \n",
    "        predictions_all_models = pd.DataFrame(columns = get_colnames(models_trained, colnames_predictions), index = genes)\n",
    "        for model in models_trained:\n",
    "    \n",
    "            file_predictions = input_predictions + model + \"/predictions_\" + dataset + \".csv\"\n",
    "            predictions = pd.read_csv(file_predictions, header=0, index_col=1)\n",
    "            predictions.sort_index(inplace=True)\n",
    "            \n",
    "            eval_top_n_guides_modelwise(model, dataset, predictions, top_n_guides_list, pp)\n",
    "        \n",
    "            for colname in colnames_predictions:\n",
    "                predictions_all_models[model + \"_\" + colname] = predictions[colname]\n",
    "        \n",
    "        pp.close()\n",
    "        \n",
    "        #calculat metics per gene\n",
    "        metrics_per_gene_per_model = pd.DataFrame(columns = get_colnames(models_trained, colnames_metrics), index = genes_unique)\n",
    "        ranking_per_gene_per_model = pd.DataFrame(columns = models_trained, index = genes_unique)\n",
    "        mean_FC_top_n_guides_per_gene_per_model = pd.DataFrame(columns = models_trained, index = genes_unique)\n",
    "        mean_mse_per_gene_per_model = pd.DataFrame(columns = models_trained, index = genes_unique)\n",
    "        spearmanR_per_gene_per_model = pd.DataFrame(columns = models_trained, index = genes_unique)\n",
    "        \n",
    "        for gene in genes_unique:\n",
    "            predictions_of_gene = predictions_all_models.loc[gene,:].copy()\n",
    "            if predictions_of_gene.shape[0] > 10:\n",
    "                metrics_per_gene_per_model, ranking_per_gene_per_model, mean_FC_top_n_guides_per_gene_per_model, mean_mse_per_gene_per_model, spearmanR_per_gene_per_model = eval_top_n_guides_genewise(top_n_guides, models_trained, dataset, predictions_of_gene, \n",
    "                                                                                                                                            metrics_per_gene_per_model.copy(), ranking_per_gene_per_model.copy(), \n",
    "                                                                                                                                            mean_FC_top_n_guides_per_gene_per_model.copy(), mean_mse_per_gene_per_model.copy(), spearmanR_per_gene_per_model.copy(),\n",
    "                                                                                                                                            colnames_metrics, gene, output_plots + dataset + \"/\", plot=True)\n",
    "\n",
    "        \n",
    "        \n",
    "        # drop lines of excluded genes\n",
    "        metrics_per_gene_per_model = metrics_per_gene_per_model.dropna()\n",
    "        ranking_per_gene_per_model = ranking_per_gene_per_model.dropna()\n",
    "        mean_FC_top_n_guides_per_gene_per_model = mean_FC_top_n_guides_per_gene_per_model.dropna()\n",
    "        mean_mse_per_gene_per_model = mean_mse_per_gene_per_model.dropna()\n",
    "        spearmanR_per_gene_per_model = spearmanR_per_gene_per_model.dropna()\n",
    "        \n",
    "        \n",
    "        # get number of significant p-values per model and calculate ranking\n",
    "        ranking_per_model_per_dataset, number_sig_p_values_per_model_per_dataset = calculate_ranking(models_trained, dataset, metrics_per_gene_per_model, p_value_thr, \n",
    "                                                                                                     ranking_per_model_per_dataset.copy(), number_sig_p_values_per_model_per_dataset.copy())\n",
    "        ranking_per_gene_per_model_mean.loc[dataset, :] = ranking_per_gene_per_model.mean().to_list()\n",
    "        \n",
    "        #create challengeR tables for mean FC of top n guides per gene\n",
    "        #mean_FC_top_n_guides_per_gene_per_model['TestCase'] = mean_FC_top_n_guides_per_gene_per_model.index\n",
    "        #mean_FC_top_n_guides_per_gene_per_model = pd.melt(mean_FC_top_n_guides_per_gene_per_model, id_vars='TestCase')\n",
    "        #mean_FC_top_n_guides_per_gene_per_model[\"Task\"] = dataset\n",
    "        \n",
    "        #mean_FC_top_n_guides_per_gene_per_model.columns = [\"TestCase\", \"Algorithm\", \"MetricValue\", \"Task\"]\n",
    "        mean_FC_top_n_guides_per_gene_per_model_per_dataset.append(create_challengeR_dataset(mean_FC_top_n_guides_per_gene_per_model, dataset))\n",
    "        \n",
    "        #create challengeR tables for mean mse per gene\n",
    "        #mean_mse_per_gene_per_model['TestCase'] = mean_mse_per_gene_per_model.index\n",
    "        #mean_mse_per_gene_per_model = pd.melt(mean_mse_per_gene_per_model, id_vars='TestCase')\n",
    "        #mean_mse_per_gene_per_model[\"Task\"] = dataset\n",
    "        \n",
    "        #mean_mse_per_gene_per_model.columns = [\"TestCase\", \"Algorithm\", \"MetricValue\", \"Task\"]\n",
    "        mean_mse_per_gene_per_model_per_dataset.append(create_challengeR_dataset(mean_mse_per_gene_per_model, dataset))\n",
    "        \n",
    "        #create challengeR tables for spearman correlation per gene\n",
    "        #spearmanR_per_gene_per_model['TestCase'] = spearmanR_per_gene_per_model.index\n",
    "        #spearmanR_per_gene_per_model = pd.melt(spearmanR_per_gene_per_model, id_vars='TestCase')\n",
    "        #spearmanR_per_gene_per_model[\"Task\"] = dataset\n",
    "        \n",
    "        #spearmanR_per_gene_per_model.columns = [\"TestCase\", \"Algorithm\", \"MetricValue\", \"Task\"]\n",
    "        spearmanR_per_gene_per_model_per_dataset.append(create_challengeR_dataset(spearmanR_per_gene_per_model, dataset))\n",
    "        \n",
    "        \n",
    "        # save metrics per dataset\n",
    "        metrics_per_gene_per_model.to_csv(output_performance + \"metrics_\" + dataset + \"_top_\" + str(top_n_guides) +\"_guides.csv\")\n",
    "\n",
    "    print(ranking_per_gene_per_model_mean)\n",
    "    \n",
    "    # write challengeR files for mean FC of top n guides per gene\n",
    "    mean_FC_top_n_guides_per_gene_per_model_per_dataset = pd.concat(mean_FC_top_n_guides_per_gene_per_model_per_dataset)\n",
    "    mean_FC_top_n_guides_per_gene_per_model_per_dataset.to_csv(output_performance + \"challengeR_ranking_top_\" + str(top_n_guides) + \".csv\", index=False)\n",
    "        \n",
    "    # write challengeR files for mean mse per gene\n",
    "    mean_mse_per_gene_per_model_per_dataset = pd.concat(mean_mse_per_gene_per_model_per_dataset)\n",
    "    mean_mse_per_gene_per_model_per_dataset.to_csv(output_performance + \"challengeR_mean_mse.csv\", index=False)\n",
    "    \n",
    "    # write challengeR files for spearman correlation per gene\n",
    "    spearmanR_per_gene_per_model_per_dataset = pd.concat(spearmanR_per_gene_per_model_per_dataset)\n",
    "    spearmanR_per_gene_per_model_per_dataset.to_csv(output_performance + \"challengeR_spearman.csv\", index=False)\n",
    "    \n",
    "        \n",
    "    # plot ranking\n",
    "    filename_plots = output_plots + \"ranking_top_\" + str(top_n_guides) + \".pdf\"\n",
    "    pp = PdfPages(filename_plots)\n",
    "    plot_ranking(ranking_per_model_per_dataset.copy(), number_sig_p_values_per_model_per_dataset.copy(), top_n_guides, pp)\n",
    "    pp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel_crispri",
   "language": "python",
   "name": "kernel_crispri"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
